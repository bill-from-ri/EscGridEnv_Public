{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from json import load, dump\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "options = {\n",
    "    'lvl' : ['lvl_1', 'lvl_3', 'lvl_5', 'lvl_9', 'lvl_11', 'lvl_12'],\n",
    "    'prompt' : ['base', 'cot'],\n",
    "    'model' : ['llama3', 'qwen', 'deepseek'],\n",
    "    'encoding' : ['matrix', 'text']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resourceful(entry):\n",
    "    return entry['contains_keyword']\n",
    "\n",
    "def interactive(entry):\n",
    "    return any(entry['results'].values())\n",
    "\n",
    "def accurate(truth):\n",
    "    return truth['tp'] > 0\n",
    "\n",
    "def complete(entry):\n",
    "    return entry['completion']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data(experiment, idp):\n",
    "\n",
    "    four_tuples = [[0, 0, 0, 0] for _ in options[idp]]\n",
    "    norm_vals   = [0 for _ in options[idp]]\n",
    "\n",
    "    for i, (e1, e2) in enumerate(zip(plan_data, action_data)):\n",
    "        if e1['meta']['exp'] == experiment and e1['meta']['prompt'] != 'self-consistency':\n",
    "\n",
    "            # Determine where in four_tuples to index.\n",
    "            option_dict = {o : i for i, o in enumerate(options[idp])}\n",
    "            idx         = option_dict[e1['meta'][idp]]\n",
    "\n",
    "            four_tuples[idx][0] += 1 if resourceful(e1) else 0\n",
    "            four_tuples[idx][1] += 1 if interactive(e2) else 0\n",
    "            four_tuples[idx][2] += 1 if accurate(acc_data[i]['truth']) else 0\n",
    "            four_tuples[idx][3] += 1 if complete(e2) else 0\n",
    "\n",
    "            norm_vals[idx] += 1\n",
    "\n",
    "    return list(zip(four_tuples, norm_vals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that performs statistical analysis.\n",
    "def do_comparison(\n",
    "        h0_succ, h1_succ,\n",
    "        h0_fail, h1_fail,\n",
    "        h0_count, h1_count,\n",
    "        metric_idx,\n",
    "        metric,\n",
    "        alpha=0.05):\n",
    "    contingency_table = np.array([\n",
    "        [h0_succ[metric_idx], h0_fail[metric_idx]],\n",
    "        [h1_succ[metric_idx], h1_fail[metric_idx]]\n",
    "    ])\n",
    "\n",
    "    # Perform chi-square test\n",
    "    chi2, p, dof, expected = stats.chi2_contingency(\n",
    "        contingency_table,\n",
    "        correction=True\n",
    "    )\n",
    "\n",
    "    # Get p-values.\n",
    "    p1 = h0_succ[metric_idx] / h0_count\n",
    "    p2 = h1_succ[metric_idx] / h1_count\n",
    "\n",
    "    # Calculate pooled proportion\n",
    "    p_pooled = (h0_succ[metric_idx] + h1_succ[metric_idx]) / (h0_count + h1_count)\n",
    "\n",
    "    # Calculate standard error\n",
    "    se = np.sqrt(p_pooled * (1 - p_pooled) * (1/h0_count + 1/h1_count))\n",
    "\n",
    "    # Calculate z-score\n",
    "    z = (p2 - p1) / se\n",
    "\n",
    "    # Calculate p-value from z-score (two-tailed)\n",
    "    p_z = 2 * (1 - stats.norm.cdf(abs(z)))\n",
    "\n",
    "    # Determine which model performed better\n",
    "    difference = p2 - p1\n",
    "    if difference > 0:\n",
    "        better_model = \"Model 2\"\n",
    "    elif difference < 0:\n",
    "        better_model = \"Model 1\"\n",
    "    else:\n",
    "        better_model = \"Neither (identical performance)\"\n",
    "\n",
    "    # Determine if difference is significant\n",
    "    is_significant = p < alpha\n",
    "\n",
    "    return {\n",
    "        'Metric Name': metric,\n",
    "        'Model 1 Rate': f\"{h0_succ[metric_idx]}/{h0_count} = {p1:.2%}\",\n",
    "        'Model 2 Rate': f\"{h1_succ[metric_idx]}/{h1_count} = {p2:.2%}\",\n",
    "        'Difference': f\"{difference:.2%}\",\n",
    "        'X^2 p-value': f\"{p:.8f}\",\n",
    "        'Z-test p-val': f\"{p_z:.8f}\",\n",
    "        'Significant': is_significant,\n",
    "        'Better Model': better_model\n",
    "    }\n",
    "\n",
    "# Function that formats data correctly then performs statistical analysis. We\n",
    "# want to test how a new idp value affects average performance across both\n",
    "# experiments.\n",
    "def format_data(data, idp, idx1, idx2, metric_idx, metric):\n",
    "    h0_data = [data['exp1'][idp][idx1], data['exp2'][idp][idx1]]\n",
    "    h1_data = [data['exp1'][idp][idx2], data['exp2'][idp][idx2]]\n",
    "\n",
    "    h0_count = sum([pair[1] for pair in h0_data])\n",
    "    h1_count = sum([pair[1] for pair in h1_data])\n",
    "\n",
    "    h0_succ = np.array(h0_data[0][0]) + np.array(h0_data[1][0])\n",
    "    h0_fail = h0_count - h0_succ\n",
    "    h1_succ = np.array(h1_data[0][0]) + np.array(h1_data[1][0])\n",
    "    h1_fail = h1_count - h1_succ\n",
    "\n",
    "    return do_comparison(\n",
    "        h0_succ, h1_succ,\n",
    "        h0_fail, h1_fail,\n",
    "        h0_count, h1_count,\n",
    "        metric_idx, metric\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract(data):\n",
    "    result = []\n",
    "    for vv in data.values():\n",
    "        result += vv\n",
    "    return result\n",
    "\n",
    "def format_data_overall(data, metric_idx, metric):\n",
    "    h0_data = extract(data['exp1'])\n",
    "    h1_data = extract(data['exp2'])\n",
    "\n",
    "    h0_count = sum([pair[1] for pair in h0_data])\n",
    "    h1_count = sum([pair[1] for pair in h1_data])\n",
    "\n",
    "    h0_succ = sum([np.array(pair[0]) for pair in h0_data])\n",
    "    h0_fail = h0_count - h0_succ\n",
    "    h1_succ = sum([np.array(pair[0]) for pair in h1_data])\n",
    "    h1_fail = h1_count - h1_succ\n",
    "\n",
    "    return do_comparison(\n",
    "        h0_succ, h1_succ,\n",
    "        h0_fail, h1_fail,\n",
    "        h0_count, h1_count,\n",
    "        metric_idx, metric\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Newly Formatted Data\n",
    "\n",
    "Requires a bit of preprocessing before throwing it into `do_comparison`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/preprocessed.json') as fp:\n",
    "    new_data = load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metric Name : \tResourcefulness\n",
      "Model 1 Rate : \t309/384 = 80.47%\n",
      "Model 2 Rate : \t247/384 = 64.32%\n",
      "Difference : \t-16.15%\n",
      "X^2 p-value : \t0.00000085\n",
      "Z-test p-val : \t0.00000056\n",
      "Significant : \tTrue\n",
      "Better Model : \tModel 1\n",
      "------------------------------------------------\n",
      "Metric Name : \tInteraction\n",
      "Model 1 Rate : \t79/384 = 20.57%\n",
      "Model 2 Rate : \t39/384 = 10.16%\n",
      "Difference : \t-10.42%\n",
      "X^2 p-value : \t0.00009519\n",
      "Z-test p-val : \t0.00006265\n",
      "Significant : \tTrue\n",
      "Better Model : \tModel 1\n",
      "------------------------------------------------\n",
      "Metric Name : \tAccuracy\n",
      "Model 1 Rate : \t52/384 = 13.54%\n",
      "Model 2 Rate : \t32/384 = 8.33%\n",
      "Difference : \t-5.21%\n",
      "X^2 p-value : \t0.02804341\n",
      "Z-test p-val : \t0.02076153\n",
      "Significant : \tTrue\n",
      "Better Model : \tModel 1\n",
      "------------------------------------------------\n",
      "Metric Name : \tCompletion\n",
      "Model 1 Rate : \t27/384 = 7.03%\n",
      "Model 2 Rate : \t11/384 = 2.86%\n",
      "Difference : \t-4.17%\n",
      "X^2 p-value : \t0.01256554\n",
      "Z-test p-val : \t0.00776223\n",
      "Significant : \tTrue\n",
      "Better Model : \tModel 1\n",
      "------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "kk = 'byModel'\n",
    "m1 = 'qwen'\n",
    "m2 = 'deepseek'\n",
    "\n",
    "for i, metric in enumerate(['Resourcefulness', 'Interaction', 'Accuracy', 'Completion']):\n",
    "\n",
    "    h0_count = new_data[kk][m1]['Total']\n",
    "    h1_count = new_data[kk][m2]['Total']\n",
    "\n",
    "    h0_succ = list(new_data[kk][m1].values())\n",
    "    h1_succ = list(new_data[kk][m2].values())\n",
    "    h0_fail = list(h0_count - np.array(h0_succ))\n",
    "    h1_fail = list(h1_count - np.array(h1_succ))\n",
    "\n",
    "    for k, v in do_comparison(h0_succ, h1_succ, h0_fail, h1_fail, h0_count, h1_count, i, metric).items():\n",
    "        print(f\"{k} : \\t{v}\")\n",
    "    print('------------------------------------------------')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
